{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETUP\n",
    "Before running anything:\n",
    "1. Get openAI API key and store as \"OPENAI_API_KEY\" in .env file (create in working directory)\n",
    "2. Get Kaggle API json file (should be called Kaggle Settings.json) and store in \".kaggle\" folder (create in working directory)\n",
    "\n",
    "Running code below will:\n",
    "1. Download 2gb Dermnet dataset\n",
    "2. Create a json file called \"diagnosis_mapping.json\" that contains each image path and diagnosis\n",
    "3. Create a persistent vectorDB called \" using chroma\n",
    "4. Store image and metadata embeddings in vectorDB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Just run this first, dont need to edit anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "import json\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"): \n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded to: C:\\Users\\tanhu\\.cache\\kagglehub\\datasets\\shubhamgoel27\\dermnet\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "# Download the dataset from Kaggle (ensure kagglehub is installed and configured)\n",
    "import kagglehub\n",
    "dataset_path = kagglehub.dataset_download(\"shubhamgoel27/dermnet\")\n",
    "print(\"Dataset downloaded to:\", dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagnosis mapping saved to diagnosis_mapping.json\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Generate Diagnosis Mapping JSON\n",
    "\n",
    "def extract_diagnosis(filename):\n",
    "    \"\"\"\n",
    "    Remove file extension and leading numbers/special characters, then split and capitalize to create a diagnosis string.\n",
    "    \"\"\"\n",
    "    name = os.path.splitext(filename)[0]\n",
    "    cleaned = re.sub(r'^[\\d_\\-]+', '', name)\n",
    "    parts = re.split(r'[\\d_\\-]+', cleaned)\n",
    "    diagnosis = ' '.join(part.strip().capitalize() for part in parts if part.strip())\n",
    "    return diagnosis\n",
    "\n",
    "def download_and_generate_json(root_dir):\n",
    "    output_file = \"diagnosis_mapping.json\"\n",
    "    data = []\n",
    "    # Recursively walk through dataset directories\n",
    "    for root, dirs, files in os.walk(root_dir):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                full_path = os.path.join(root, file)\n",
    "                diagnosis = extract_diagnosis(file)\n",
    "                data.append({\n",
    "                    \"path\": full_path,\n",
    "                    \"diagnosis\": diagnosis\n",
    "                })\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "    print(f\"Diagnosis mapping saved to {output_file}\")\n",
    "    return output_file\n",
    "\n",
    "json_path = download_and_generate_json(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 19559 documents.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Create Document objects from the JSON data\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "def create_documents_from_json(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    docs = []\n",
    "    for entry in data:\n",
    "        # For embedding, we use the diagnosis text; image path is stored in metadata\n",
    "        doc = Document(page_content=entry['diagnosis'], metadata=entry)\n",
    "        docs.append(doc)\n",
    "    return docs\n",
    "\n",
    "docs = create_documents_from_json(json_path)\n",
    "print(f\"Created {len(docs)} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tanhu\\AppData\\Local\\Temp\\ipykernel_45632\\3955983089.py:6: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of docs in vector store: 19559\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Set up IRISVector with OpenAI Embeddings\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain_iris import IRISVector\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Define your IRIS connection parameters (adjust as needed)\n",
    "username = 'demo'\n",
    "password = 'demo'\n",
    "hostname = os.getenv('IRIS_HOSTNAME', 'localhost')\n",
    "port = '1972'\n",
    "namespace = 'USER'\n",
    "CONNECTION_STRING = f\"iris://{username}:{password}@{hostname}:{port}/{namespace}\"\n",
    "\n",
    "# Choose a collection name (avoid dots since under the hood it becomes a SQL table)\n",
    "COLLECTION_NAME = \"dermnet_collection\"\n",
    "\n",
    "# Create (or update) the IRISVector persistent store from the documents\n",
    "db = IRISVector.from_documents(\n",
    "    embedding=embeddings,\n",
    "    documents=docs,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection_string=CONNECTION_STRING,\n",
    ")\n",
    "print(\"Connction String:\", CONNECTION_STRING)\n",
    "\n",
    "# Verify the number of documents stored\n",
    "ids = db.get().get(\"ids\", [])\n",
    "print(f\"Number of docs in vector store: {len(ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top similar results:\n",
      "Diagnosis: Acne | Path: C:\\Users\\tanhu\\.cache\\kagglehub\\datasets\\shubhamgoel27\\dermnet\\versions\\1\\train\\Psoriasis pictures Lichen Planus and related diseases\\07acne06270532.jpg\n",
      "Diagnosis: Acne | Path: C:\\Users\\tanhu\\.cache\\kagglehub\\datasets\\shubhamgoel27\\dermnet\\versions\\1\\train\\Acne and Rosacea Photos\\07Acne081101.jpg\n",
      "Diagnosis: Acne Scar | Path: C:\\Users\\tanhu\\.cache\\kagglehub\\datasets\\shubhamgoel27\\dermnet\\versions\\1\\test\\Acne and Rosacea Photos\\acne-scar-2.jpg\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Query the vector store\n",
    "\n",
    "# For IRISVector, we query using a text string. For example, if you want to search for similar entries related to \"Acne\":\n",
    "query_text = \"Acne\"\n",
    "results = db.similarity_search(query_text, k=3)\n",
    "\n",
    "print(\"Top similar results:\")\n",
    "for result in results:\n",
    "    # Each result is a Document object; metadata contains the original image path and diagnosis\n",
    "    diagnosis = result.metadata.get(\"diagnosis\", \"N/A\")\n",
    "    path = result.metadata.get(\"path\", \"N/A\")\n",
    "    print(f\"Diagnosis: {diagnosis} | Path: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edit code below and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DB_path = \"dermnetVectorDB\"\n",
    "# data_path = \"./diagnosis_mapping.json\"\n",
    "\n",
    "# create_and_add_to_DB(DB_path, data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To check if it works, run a vector search query, make edits accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multimodal_ef = OpenCLIPEmbeddingFunction() # multimodal embedding function\n",
    "# image_loader = ImageLoader() # multimodal data loader\n",
    "# # client = chromadb.Client() # non-persistent DB\n",
    "# client = chromadb.PersistentClient(path=DB_path) # edit to your own path\n",
    "\n",
    "# multimodalDB = client.get_or_create_collection(name=\"multimodalDB\", embedding_function= multimodal_ef, data_loader=image_loader) # multimodal collection\n",
    "\n",
    "# # Query/search n most similar items\n",
    "# results = multimodalDB.query(\n",
    "#     query_uris=[\"/Users/shinherng/Downloads/skinCond9.jpg\"], # edit to your own test image\n",
    "#     n_results=3\n",
    "# )\n",
    "\n",
    "# results[\"metadatas\"][0][0][\"Diagnosis\"] # to access top 1 similar diagnosis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
