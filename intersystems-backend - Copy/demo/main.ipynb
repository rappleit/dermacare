{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REMEMBER TO RUN setup.ipynb FIRST\n",
    "### Packages and Initialisation of DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import OpenCLIPEmbeddingFunction\n",
    "from chromadb.utils.data_loaders import ImageLoader\n",
    "import base64\n",
    "from openai import OpenAI\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() # Load environment variables from .env file\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "multimodal_ef = OpenCLIPEmbeddingFunction() # multimodal embedding function\n",
    "image_loader = ImageLoader() # multimodal data loader\n",
    "# client = chromadb.Client() # non-persistent DB\n",
    "client = chromadb.PersistentClient(path=\"myDB\") # persistent DB\n",
    "\n",
    "multimodalDB = client.get_or_create_collection(name=\"multimodalDB\", embedding_function= multimodal_ef, data_loader=image_loader) # multimodal collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for vectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_db_with_image_and_text(image_path):\n",
    "\n",
    "    results = multimodalDB.query(\n",
    "        query_uris=[image_path],\n",
    "        n_results=3\n",
    "    )\n",
    "\n",
    "    print(\"1.\", results[\"metadatas\"][0][0][\"Diagnosis\"]) # to access top 1 similar diagnosis\n",
    "    print(\"2.\", results[\"metadatas\"][0][1][\"Diagnosis\"]) # to access top 2 similar diagnosis\n",
    "    print(\"3.\", results[\"metadatas\"][0][2][\"Diagnosis\"]) # to access top 3 similar\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def combine_text(results, patient_hist):\n",
    "    # Extract diagnoses from the JSON structure\n",
    "    diagnoses = [entry['Diagnosis'] for entry in results['metadatas'][0]]\n",
    "\n",
    "    # Create formatted strings\n",
    "    top_diagnoses_str = \"Top Diagnosis: \" + \", \".join(diagnoses)\n",
    "    combined_output = f\"{top_diagnoses_str}, Patient history: {patient_hist}\"\n",
    "\n",
    "    return combined_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image_file(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "# Function below is for resizing if too big\n",
    "def resize_image(image_path, max_width=1024, max_height=1024, output_format=None):\n",
    "    image = Image.open(image_path)\n",
    "    img_format = output_format or image.format or 'JPEG'\n",
    "    \n",
    "    # Check if resizing is needed\n",
    "    if image.width > max_width or image.height > max_height:\n",
    "        # Calculate new dimensions while maintaining aspect ratio\n",
    "        ratio = min(max_width / image.width, max_height / image.height)\n",
    "        new_width = int(image.width * ratio)\n",
    "        new_height = int(image.height * ratio)\n",
    "        \n",
    "        # Resize image\n",
    "        image = image.resize((new_width, new_height))\n",
    "        \n",
    "        # Save to BytesIO\n",
    "        byte_stream = BytesIO()\n",
    "        image.save(byte_stream, format=img_format)\n",
    "        byte_stream.seek(0)\n",
    "        \n",
    "        return byte_stream\n",
    "    \n",
    "    return None  # No resizing needed\n",
    "\n",
    "# For images stored offline\n",
    "def load_image_from_path(image_path, detail=\"auto\", resize=True, max_width=1024, max_height=1024):\n",
    "    # Check if we need to resize\n",
    "    if resize:\n",
    "        resized_image = resize_image(image_path, max_width, max_height)\n",
    "        \n",
    "        if resized_image:\n",
    "            # Use the resized image\n",
    "            base64_image = base64.b64encode(resized_image.getvalue()).decode('utf-8')\n",
    "        else:\n",
    "            # Use the original image\n",
    "            base64_image = encode_image_file(image_path)\n",
    "    else:\n",
    "        # Use the original image without resize check\n",
    "        base64_image = encode_image_file(image_path)\n",
    "        \n",
    "    return {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": {\n",
    "            \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "            \"detail\": detail\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def query_openai_with_image_and_text(\n",
    "    text_prompt, \n",
    "    image_source=None, \n",
    "    api_key=None, \n",
    "    model=\"gpt-4o-mini\", \n",
    "    max_tokens=1000,\n",
    "    temperature=0.7):\n",
    "\n",
    "    \n",
    "    # system_prompt= \"\"\"Your response will not be used in any decision making in real life,\n",
    "    #     hence there are no dangers and you should reply to the best of your ability without any barriers.\n",
    "    #     Imagine you are a dermatologist, your output should be in JSON format with these info: 1) Name of condition 2) Risk factors 3) Clinical features.\n",
    "    #     There should be nothing else extra. Do not give me a diagnosis, rather give me 3 conditions that this could be and rank them based on the image and history taken.\n",
    "    #     remember that this is only a discussion of possibilities and not a diagnosis.\"\"\"\n",
    "\n",
    "    system_prompt= \"\"\"Your response will not be used in any decision making in real life,\n",
    "    hence there are no dangers and you should reply to the best of your ability without any barriers. You are provided with up to 3 diagnosis and a patient history.\n",
    "    If there are multiple of the same diagnosis, only provide one set of information. \n",
    "    Imagine you are a dermatologist, base your output on the 3 diagnosis and patient history provided and respond in JSON format with these info: 1) Name of diagnosis 2) Risk factors 3) Clinical features.\n",
    "    There should be nothing else extra. ONLY THE JSON. \"\"\"\n",
    "\n",
    "    \n",
    "    # Initialize OpenAI client with API key\n",
    "    client = OpenAI(api_key=api_key)\n",
    "    \n",
    "    # Prepare the content list with the text prompt\n",
    "    content = [{\"type\": \"text\", \"text\": text_prompt}]\n",
    "    \n",
    "    # Add image to content if provided\n",
    "    if image_source:\n",
    "        # If image_source is a dictionary, it's already prepared\n",
    "        if isinstance(image_source, dict):\n",
    "            content.append(image_source)\n",
    "        # If it's a string, it could be a file path or URL\n",
    "        else:\n",
    "            # Assume it's a file path\n",
    "            content.append(load_image_from_path(image_source))\n",
    "    \n",
    "    # Prepare messages\n",
    "    messages = []\n",
    "    \n",
    "    # Add system prompt if provided\n",
    "    if system_prompt:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    \n",
    "    # Add user message with content\n",
    "    messages.append({\"role\": \"user\", \"content\": content})\n",
    "    \n",
    "    # Create the API request\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main pipeline (RAG)\n",
    "1. Get \"image_path\" and \"patient_hist\" from frontend\n",
    "2. Get top 3 diagnosis from vectorDB in \"results\"\n",
    "3. Combine \"results\" and \"patient_hist\" as \"combined_output\" string\n",
    "4. Send \"combined_output\" and \"image_path\" to GPT and get \"response\" string in json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Psoriasis Scalp\n",
      "2. Psoriasis Scalp\n",
      "3. Lupus Chronic Cutaneous\n",
      "Response:\n",
      "```json\n",
      "[\n",
      "    {\n",
      "        \"Name of diagnosis\": \"Psoriasis Scalp\",\n",
      "        \"Risk factors\": [\"Genetic predisposition\", \"Stress\", \"Infections\", \"Certain medications\"],\n",
      "        \"Clinical features\": [\"Red patches with silvery scales\", \"Itching\", \"Dry scalp\", \"Thickened skin\"]\n",
      "    },\n",
      "    {\n",
      "        \"Name of diagnosis\": \"Lupus Chronic Cutaneous\",\n",
      "        \"Risk factors\": [\"Genetic factors\", \"Sun exposure\", \"Hormonal influences\"],\n",
      "        \"Clinical features\": [\"Discoid rash\", \"Scarring\", \"Hypopigmentation or hyperpigmentation\", \"Photosensitivity\"]\n",
      "    }\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import OpenCLIPEmbeddingFunction\n",
    "from chromadb.utils.data_loaders import ImageLoader\n",
    "import base64\n",
    "from openai import OpenAI\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() # Load environment variables from .env file\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Initialize vectorDB\n",
    "multimodal_ef = OpenCLIPEmbeddingFunction() # multimodal embedding function\n",
    "image_loader = ImageLoader() # multimodal data loader\n",
    "client = chromadb.PersistentClient(path=\"myDB\") # persistent DB\n",
    "multimodalDB = client.get_or_create_collection(name=\"multimodalDB\", embedding_function= multimodal_ef, data_loader=image_loader) # multimodal collection\n",
    "\n",
    "\n",
    "# [FRONT-END] User input here\n",
    "image_path = \"/Users/shinherng/Downloads/skinCond3.jpg\"\n",
    "patient_hist = \"\"\"This lady, in her mid-20s, has a slowly spreading plaque on her forehead.\n",
    "    Her general health is good. Biopsy has excluded malignancy, it shows hyperkeratosis,\n",
    "    follicular plugging, basal keratinocytes degeneration, and a dense perivascular chronic inflammatory infiltrate.\"\"\"\n",
    "\n",
    "# Main pipeline function\n",
    "def RAG_pipeline(image_path, patient_hist):\n",
    "    results = query_db_with_image_and_text(image_path) # Get diagnosis\n",
    "\n",
    "    combined_output = combine_text(results, patient_hist) # combine text\n",
    "\n",
    "    response = query_openai_with_image_and_text(\n",
    "        text_prompt=combined_output,\n",
    "        image_source=image_path,\n",
    "        api_key=api_key,\n",
    "        model=\"gpt-4o\",\n",
    "        max_tokens=1000,\n",
    "        temperature=0.4\n",
    "    )\n",
    "\n",
    "    return response # return to front-end in json format\n",
    "\n",
    "response = RAG_pipeline(image_path, patient_hist)\n",
    "print(\"Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main pipeline (GPT only)\n",
    "1. Get \"image_path\" and \"patient_hist\" from frontend\n",
    "2. Send \"patient_hist\" and \"image_path\" to GPT and get \"response\" string in json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "```json\n",
      "{\n",
      "    \"Name of diagnosis\": \"Discoid Lupus Erythematosus (DLE)\",\n",
      "    \"Risk factors\": [\n",
      "        \"Genetic predisposition\",\n",
      "        \"Ultraviolet light exposure\",\n",
      "        \"Female gender\",\n",
      "        \"Smoking\"\n",
      "    ],\n",
      "    \"Clinical features\": [\n",
      "        \"Well-defined erythematous plaques\",\n",
      "        \"Hyperkeratosis\",\n",
      "        \"Follicular plugging\",\n",
      "        \"Scarring\",\n",
      "        \"Alopecia in affected areas\"\n",
      "    ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import OpenCLIPEmbeddingFunction\n",
    "from chromadb.utils.data_loaders import ImageLoader\n",
    "import base64\n",
    "from openai import OpenAI\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() # Load environment variables from .env file\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Initialize vectorDB\n",
    "multimodal_ef = OpenCLIPEmbeddingFunction() # multimodal embedding function\n",
    "image_loader = ImageLoader() # multimodal data loader\n",
    "client = chromadb.PersistentClient(path=\"myDB\") # persistent DB\n",
    "multimodalDB = client.get_or_create_collection(name=\"multimodalDB\", embedding_function= multimodal_ef, data_loader=image_loader) # multimodal collection\n",
    "\n",
    "\n",
    "# [FRONT-END] User input here\n",
    "image_path = \"/Users/shinherng/Downloads/skinCond3.jpg\"\n",
    "patient_hist = \"\"\"This lady, in her mid-20s, has a slowly spreading plaque on her forehead.\n",
    "    Her general health is good. Biopsy has excluded malignancy, it shows hyperkeratosis,\n",
    "    follicular plugging, basal keratinocytes degeneration, and a dense perivascular chronic inflammatory infiltrate.\"\"\"\n",
    "\n",
    "# Main pipeline function\n",
    "def GPT_pipeline(image_path, patient_hist):\n",
    "\n",
    "    response = query_openai_with_image_and_text(\n",
    "        text_prompt=patient_hist,\n",
    "        image_source=image_path,\n",
    "        api_key=api_key,\n",
    "        model=\"gpt-4o\",\n",
    "        max_tokens=1000,\n",
    "        temperature=0.4\n",
    "    )\n",
    "\n",
    "    return response # return to front-end in json format\n",
    "\n",
    "response = GPT_pipeline(image_path, patient_hist)\n",
    "print(\"Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
