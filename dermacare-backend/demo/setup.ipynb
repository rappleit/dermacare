{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETUP\n",
    "Before running anything:\n",
    "1. Get openAI API key and store as \"OPENAI_API_KEY\" in .env file (create in working directory)\n",
    "2. Get Kaggle API json file (should be called Kaggle Settings.json) and store in \".kaggle\" folder (create in working directory)\n",
    "\n",
    "Running code below will:\n",
    "1. Download 2gb Dermnet dataset\n",
    "2. Create a json file called \"diagnosis_mapping.json\" that contains each image path and diagnosis\n",
    "3. Create a persistent vectorDB called \" using chroma\n",
    "4. Store image and metadata embeddings in vectorDB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Just run this first, dont need to edit anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "import json\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "from PIL import Image\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"): \n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded to: C:\\Users\\tanhu\\.cache\\kagglehub\\datasets\\shubhamgoel27\\dermnet\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "# Download the dataset from Kaggle (ensure kagglehub is installed and configured)\n",
    "import kagglehub\n",
    "dataset_path = kagglehub.dataset_download(\"shubhamgoel27/dermnet\")\n",
    "print(\"Dataset downloaded to:\", dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 19559 image documents.\n"
     ]
    }
   ],
   "source": [
    "# --- Create Documents from Image Files ---\n",
    "def extract_diagnosis(filename):\n",
    "    \"\"\"\n",
    "    Extracts a diagnosis string from a filename by removing leading numbers/special characters\n",
    "    and capitalizing the remaining words.\n",
    "    \"\"\"\n",
    "    name = os.path.splitext(filename)[0]\n",
    "    cleaned = re.sub(r'^[\\d_\\-]+', '', name)\n",
    "    parts = re.split(r'[\\d_\\-]+', cleaned)\n",
    "    diagnosis = ' '.join(part.strip().capitalize() for part in parts if part.strip())\n",
    "    return diagnosis\n",
    "\n",
    "def create_documents_from_images(root_dir):\n",
    "    \"\"\"\n",
    "    Walk through the dataset directory and create a Document for each image.\n",
    "    The document's page_content is set to the image file path and its metadata contains the diagnosis.\n",
    "    \"\"\"\n",
    "    from langchain.docstore.document import Document\n",
    "    docs = []\n",
    "    for root, dirs, files in os.walk(root_dir):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                full_path = os.path.join(root, file)\n",
    "                diagnosis = extract_diagnosis(file)\n",
    "                doc = Document(page_content=full_path, metadata={\"diagnosis\": diagnosis, \"path\": full_path})\n",
    "                docs.append(doc)\n",
    "    return docs\n",
    "\n",
    "docs = create_documents_from_images(dataset_path)\n",
    "print(f\"Created {len(docs)} image documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 5 documents for the small batch test.\n",
      "Vector store created with image embeddings for the small batch.\n",
      "Number of docs in vector store: 20\n",
      "Time taken: 3.59 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from langchain_experimental.open_clip import OpenCLIPEmbeddings\n",
    "from langchain_iris import IRISVector\n",
    "import os\n",
    "\n",
    "# Initialize the image embedding function with your chosen model and checkpoint.\n",
    "multimodal_ef = OpenCLIPEmbeddings(model_name=\"ViT-g-14\", checkpoint=\"laion2b_s34b_b88k\")\n",
    "\n",
    "# Define your IRIS connection parameters.\n",
    "username = 'demo'\n",
    "password = 'demo'\n",
    "hostname = os.getenv('IRIS_HOSTNAME', 'localhost')\n",
    "port = '1972'\n",
    "namespace = 'USER'\n",
    "CONNECTION_STRING = f\"iris://{username}:{password}@{hostname}:{port}/{namespace}\"\n",
    "\n",
    "# Choose a collection name (avoid periods as it becomes a SQL table name).\n",
    "COLLECTION_NAME = \"dermnet_multimodal_small_batch\"\n",
    "\n",
    "# Use a small batch (first 5 documents) for testing.\n",
    "small_docs = docs[:5]\n",
    "print(f\"Using {len(small_docs)} documents for the small batch test.\")\n",
    "\n",
    "# Optionally time the operation.\n",
    "start = time.time()\n",
    "\n",
    "# Create or update the IRISVector persistent vector store with the small batch of image documents.\n",
    "db = IRISVector.from_documents(\n",
    "    embedding=multimodal_ef,\n",
    "    documents=small_docs,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection_string=CONNECTION_STRING,\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "print(\"Vector store created with image embeddings for the small batch.\")\n",
    "ids = db.get().get(\"ids\", [])\n",
    "print(f\"Number of docs in vector store: {len(ids)}\")\n",
    "print(f\"Time taken: {elapsed:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top similar results:\n",
      "Diagnosis: Rhinophyma | Path: C:\\Users\\tanhu\\.cache\\kagglehub\\datasets\\shubhamgoel27\\dermnet\\versions\\1\\test\\Acne and Rosacea Photos\\07Rhinophyma1.jpg\n",
      "Diagnosis: Rhinophyma | Path: C:\\Users\\tanhu\\.cache\\kagglehub\\datasets\\shubhamgoel27\\dermnet\\versions\\1\\test\\Acne and Rosacea Photos\\07Rhinophyma1.jpg\n",
      "Diagnosis: Rhinophyma | Path: C:\\Users\\tanhu\\.cache\\kagglehub\\datasets\\shubhamgoel27\\dermnet\\versions\\1\\test\\Acne and Rosacea Photos\\07Rhinophyma1.jpg\n"
     ]
    }
   ],
   "source": [
    "# --- Query the Vector Store Using an Image ---\n",
    "# Update this path to point to your query image.\n",
    "query_image_path = \"../data/test.jpg\"\n",
    "# Embed the query image. Note: embed_image expects a list of image URIs.\n",
    "query_embedding = multimodal_ef.embed_image([query_image_path])[0]\n",
    "\n",
    "# Perform a similarity search using the query image's embedding vector.\n",
    "results = db.similarity_search_by_vector(query_embedding, k=3)\n",
    "\n",
    "print(\"Top similar results:\")\n",
    "for result in results:\n",
    "    diagnosis = result.metadata.get(\"diagnosis\", \"N/A\")\n",
    "    path = result.metadata.get(\"path\", \"N/A\")\n",
    "    print(f\"Diagnosis: {diagnosis} | Path: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top similar results:\n",
      "Diagnosis: Rhinophyma | Path: C:\\Users\\tanhu\\.cache\\kagglehub\\datasets\\shubhamgoel27\\dermnet\\versions\\1\\test\\Acne and Rosacea Photos\\07Rhinophyma1.jpg\n",
      "Diagnosis: Rhinophyma | Path: C:\\Users\\tanhu\\.cache\\kagglehub\\datasets\\shubhamgoel27\\dermnet\\versions\\1\\test\\Acne and Rosacea Photos\\07Rhinophyma1.jpg\n",
      "Diagnosis: Rhinophyma | Path: C:\\Users\\tanhu\\.cache\\kagglehub\\datasets\\shubhamgoel27\\dermnet\\versions\\1\\test\\Acne and Rosacea Photos\\07Rhinophyma1.jpg\n"
     ]
    }
   ],
   "source": [
    "## REQUERY\n",
    "from langchain_experimental.open_clip import OpenCLIPEmbeddings\n",
    "from langchain_iris import IRISVector\n",
    "import os\n",
    "\n",
    "# Initialize the embedding function\n",
    "multimodal_ef = OpenCLIPEmbeddings(model_name=\"ViT-g-14\", checkpoint=\"laion2b_s34b_b88k\")\n",
    "\n",
    "# Set up your connection parameters again\n",
    "username = 'demo'\n",
    "password = 'demo'\n",
    "hostname = os.getenv('IRIS_HOSTNAME', 'localhost')\n",
    "port = '1972'\n",
    "namespace = 'USER'\n",
    "CONNECTION_STRING = f\"iris://{username}:{password}@{hostname}:{port}/{namespace}\"\n",
    "COLLECTION_NAME = \"dermnet_multimodal_small_batch\"\n",
    "\n",
    "# Load the persistent vector store\n",
    "db = IRISVector.from_documents(\n",
    "    embedding=multimodal_ef,\n",
    "    documents=[],  # Passing an empty list so that it doesn't re-embed\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection_string=CONNECTION_STRING,\n",
    ")\n",
    "\n",
    "# --- Query the Vector Store Using an Image ---\n",
    "query_image_path = \"../data/test.jpg\"\n",
    "query_embedding = multimodal_ef.embed_image([query_image_path])[0]\n",
    "\n",
    "results = db.similarity_search_by_vector(query_embedding, k=3)\n",
    "\n",
    "print(\"Top similar results:\")\n",
    "for result in results:\n",
    "    diagnosis = result.metadata.get(\"diagnosis\", \"N/A\")\n",
    "    path = result.metadata.get(\"path\", \"N/A\")\n",
    "    print(f\"Diagnosis: {diagnosis} | Path: {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
