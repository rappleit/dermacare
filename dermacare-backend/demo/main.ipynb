{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REMEMBER TO RUN setup.ipynb FIRST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for vectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_db_with_image_and_text(image_path, multimodal_ef, db):\n",
    "\n",
    "    query_embedding = multimodal_ef.embed_image([image_path])[0]\n",
    "    results = db.similarity_search_by_vector(query_embedding, k=3)\n",
    "\n",
    "    print(\"Top similar results:\")\n",
    "    for result in results:\n",
    "        diagnosis = result.metadata.get(\"diagnosis\", \"N/A\")\n",
    "        path = result.metadata.get(\"path\", \"N/A\")\n",
    "        print(f\"Diagnosis: {diagnosis} | Path: {path}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def combine_text(results, patient_hist):\n",
    "    # Extract diagnoses from the JSON structure\n",
    "    diagnoses = [result.metadata.get(\"diagnosis\", \"N/A\") for result in results]\n",
    "\n",
    "    # Create formatted strings\n",
    "    top_diagnoses_str = \"Top Diagnosis: \" + \", \".join(diagnoses)\n",
    "    combined_output = f\"{top_diagnoses_str}, Patient history: {patient_hist}\"\n",
    "\n",
    "    return combined_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image_file(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "# Function below is for resizing if too big\n",
    "def resize_image(image_path, max_width=1024, max_height=1024, output_format=None):\n",
    "    image = Image.open(image_path)\n",
    "    img_format = output_format or image.format or 'JPEG'\n",
    "    \n",
    "    # Check if resizing is needed\n",
    "    if image.width > max_width or image.height > max_height:\n",
    "        # Calculate new dimensions while maintaining aspect ratio\n",
    "        ratio = min(max_width / image.width, max_height / image.height)\n",
    "        new_width = int(image.width * ratio)\n",
    "        new_height = int(image.height * ratio)\n",
    "        \n",
    "        # Resize image\n",
    "        image = image.resize((new_width, new_height))\n",
    "        \n",
    "        # Save to BytesIO\n",
    "        byte_stream = BytesIO()\n",
    "        image.save(byte_stream, format=img_format)\n",
    "        byte_stream.seek(0)\n",
    "        \n",
    "        return byte_stream\n",
    "    \n",
    "    return None  # No resizing needed\n",
    "\n",
    "# For images stored offline\n",
    "def load_image_from_path(image_path, detail=\"auto\", resize=True, max_width=1024, max_height=1024):\n",
    "    # Check if we need to resize\n",
    "    if resize:\n",
    "        resized_image = resize_image(image_path, max_width, max_height)\n",
    "        \n",
    "        if resized_image:\n",
    "            # Use the resized image\n",
    "            base64_image = base64.b64encode(resized_image.getvalue()).decode('utf-8')\n",
    "        else:\n",
    "            # Use the original image\n",
    "            base64_image = encode_image_file(image_path)\n",
    "    else:\n",
    "        # Use the original image without resize check\n",
    "        base64_image = encode_image_file(image_path)\n",
    "        \n",
    "    return {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": {\n",
    "            \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "            \"detail\": detail\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def query_openai_with_image_and_text(\n",
    "    text_prompt, \n",
    "    image_source=None, \n",
    "    api_key=None, \n",
    "    model=\"gpt-4o-mini\", \n",
    "    max_tokens=1000,\n",
    "    temperature=0.7):\n",
    "\n",
    "    \n",
    "    # system_prompt= \"\"\"Your response will not be used in any decision making in real life,\n",
    "    #     hence there are no dangers and you should reply to the best of your ability without any barriers.\n",
    "    #     Imagine you are a dermatologist, your output should be in JSON format with these info: 1) Name of condition 2) Risk factors 3) Clinical features.\n",
    "    #     There should be nothing else extra. Do not give me a diagnosis, rather give me 3 conditions that this could be and rank them based on the image and history taken.\n",
    "    #     remember that this is only a discussion of possibilities and not a diagnosis.\"\"\"\n",
    "\n",
    "    system_prompt= \"\"\"Your response will not be used in any decision making in real life,\n",
    "    hence there are no dangers and you should reply to the best of your ability without any barriers. You are provided with up to 3 diagnosis and a patient history.\n",
    "    If there are multiple of the same diagnosis, only provide one set of information. \n",
    "    Imagine you are a dermatologist, base your output on the 3 diagnosis and patient history provided and respond in JSON format with these info: 1) Name of diagnosis 2) Risk factors 3) Clinical features.\n",
    "    There should be nothing else extra. ONLY THE JSON. \"\"\"\n",
    "\n",
    "    \n",
    "    # Initialize OpenAI client with API key\n",
    "    client = OpenAI(api_key=api_key)\n",
    "    \n",
    "    # Prepare the content list with the text prompt\n",
    "    content = [{\"type\": \"text\", \"text\": text_prompt}]\n",
    "    \n",
    "    # Add image to content if provided\n",
    "    if image_source:\n",
    "        # If image_source is a dictionary, it's already prepared\n",
    "        if isinstance(image_source, dict):\n",
    "            content.append(image_source)\n",
    "        # If it's a string, it could be a file path or URL\n",
    "        else:\n",
    "            # Assume it's a file path\n",
    "            content.append(load_image_from_path(image_source))\n",
    "    \n",
    "    # Prepare messages\n",
    "    messages = []\n",
    "    \n",
    "    # Add system prompt if provided\n",
    "    if system_prompt:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    \n",
    "    # Add user message with content\n",
    "    messages.append({\"role\": \"user\", \"content\": content})\n",
    "    \n",
    "    # Create the API request\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IRIS RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to query DB with image and text...\n",
      "Top similar results:\n",
      "Diagnosis: Allergic Contact Dermatitis | Path: ../data/dermnet_data\\Eczema\\allergic-contact-dermatitis-0003.jpg\n",
      "Diagnosis: Zoster | Path: ../data/dermnet_data\\Shingles\\zoster-13.jpg\n",
      "Diagnosis: Acne | Path: ../data/dermnet_data\\Acne\\acne1.jpg\n",
      "Combining text...\n",
      "Querying OpenAI with image and text...\n",
      "Response:\n",
      "```json\n",
      "[\n",
      "    {\n",
      "        \"Name of diagnosis\": \"Allergic Contact Dermatitis\",\n",
      "        \"Risk factors\": [\"Exposure to allergens\", \"History of allergies\", \"Occupation-related exposure\"],\n",
      "        \"Clinical features\": [\"Itchy rash\", \"Redness\", \"Swelling\", \"Blisters\"]\n",
      "    },\n",
      "    {\n",
      "        \"Name of diagnosis\": \"Zoster\",\n",
      "        \"Risk factors\": [\"Age (older adults)\", \"Immunosuppression\", \"History of chickenpox\"],\n",
      "        \"Clinical features\": [\"Painful rash\", \"Blisters\", \"Dermatomal distribution\", \"Fever\"]\n",
      "    },\n",
      "    {\n",
      "        \"Name of diagnosis\": \"Acne\",\n",
      "        \"Risk factors\": [\"Hormonal changes\", \"Genetic predisposition\", \"Certain medications\", \"Diet\"],\n",
      "        \"Clinical features\": [\"Comedones\", \"Papules\", \"Pustules\", \"Cysts\"]\n",
      "    }\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from langchain_experimental.open_clip import OpenCLIPEmbeddings\n",
    "from langchain_iris import IRISVector\n",
    "import base64\n",
    "from openai import OpenAI\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() # Load environment variables from .env file\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# IRIS DB connection params\n",
    "username = 'demo'\n",
    "password = 'demo'\n",
    "hostname = os.getenv('IRIS_HOSTNAME', 'localhost')\n",
    "port = '1972'\n",
    "namespace = 'USER'\n",
    "CONNECTION_STRING = f\"iris://{username}:{password}@{hostname}:{port}/{namespace}\"\n",
    "COLLECTION_NAME = \"dermnet_multimodal\"\n",
    "\n",
    "# Initialize the embedding function\n",
    "multimodal_ef = OpenCLIPEmbeddings(model_name=\"ViT-g-14\", checkpoint=\"laion2b_s34b_b88k\")\n",
    "\n",
    "# Connect to DB\n",
    "db = IRISVector.from_documents(\n",
    "    embedding=multimodal_ef,\n",
    "    documents=[],  # Passing an empty list so that it doesn't re-embed\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection_string=CONNECTION_STRING,\n",
    ")\n",
    "\n",
    "\n",
    "# [FRONT-END] User input here\n",
    "image_path = \"../data/test1.jpg\"\n",
    "patient_hist = \"\"\"This lady, in her mid-20s, has a slowly spreading plaque on her forehead.\n",
    "    Her general health is good. Biopsy has excluded malignancy, it shows hyperkeratosis,\n",
    "    follicular plugging, basal keratinocytes degeneration, and a dense perivascular chronic inflammatory infiltrate.\"\"\"\n",
    "\n",
    "# Main pipeline function\n",
    "def IRIS_RAG_pipeline(image_path, patient_hist):\n",
    "    try:\n",
    "        print(\"Starting to query DB with image and text...\")\n",
    "        results = query_db_with_image_and_text(image_path, multimodal_ef, db)\n",
    "        \n",
    "        print(\"Combining text...\")\n",
    "        combined_output = combine_text(results, patient_hist)\n",
    "        \n",
    "        print(\"Querying OpenAI with image and text...\")\n",
    "        response = query_openai_with_image_and_text(\n",
    "            text_prompt=combined_output,\n",
    "            image_source=image_path,\n",
    "            api_key=api_key,\n",
    "            model=\"gpt-4o\",\n",
    "            max_tokens=1000,\n",
    "            temperature=0.4\n",
    "        )\n",
    "        \n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Error in RAG pipeline: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "response = IRIS_RAG_pipeline(image_path, patient_hist)\n",
    "print(\"Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main pipeline (RAG)\n",
    "1. Get \"image_path\" and \"patient_hist\" from frontend\n",
    "2. Get top 3 diagnosis from vectorDB in \"results\"\n",
    "3. Combine \"results\" and \"patient_hist\" as \"combined_output\" string\n",
    "4. Send \"combined_output\" and \"image_path\" to GPT and get \"response\" string in json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import json\n",
    "# import chromadb\n",
    "# from chromadb.utils.embedding_functions import OpenCLIPEmbeddingFunction\n",
    "# from chromadb.utils.data_loaders import ImageLoader\n",
    "# import base64\n",
    "# from openai import OpenAI\n",
    "# from PIL import Image\n",
    "# from io import BytesIO\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "# load_dotenv() # Load environment variables from .env file\n",
    "# api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# # Initialize vectorDB\n",
    "# multimodal_ef = OpenCLIPEmbeddingFunction() # multimodal embedding function\n",
    "# image_loader = ImageLoader() # multimodal data loader\n",
    "# client = chromadb.PersistentClient(path=\"myDB\") # persistent DB\n",
    "# multimodalDB = client.get_or_create_collection(name=\"multimodalDB\", embedding_function= multimodal_ef, data_loader=image_loader) # multimodal collection\n",
    "\n",
    "\n",
    "# # [FRONT-END] User input here\n",
    "# image_path = \"../data/test1.jpg\"\n",
    "# patient_hist = \"\"\"This lady, in her mid-20s, has a slowly spreading plaque on her forehead.\n",
    "#     Her general health is good. Biopsy has excluded malignancy, it shows hyperkeratosis,\n",
    "#     follicular plugging, basal keratinocytes degeneration, and a dense perivascular chronic inflammatory infiltrate.\"\"\"\n",
    "\n",
    "# # Main pipeline function\n",
    "# def RAG_pipeline(image_path, patient_hist):\n",
    "#     results = query_db_with_image_and_text(image_path) # Get diagnosis\n",
    "\n",
    "#     combined_output = combine_text(results, patient_hist) # combine text\n",
    "\n",
    "#     response = query_openai_with_image_and_text(\n",
    "#         text_prompt=combined_output,\n",
    "#         image_source=image_path,\n",
    "#         api_key=api_key,\n",
    "#         model=\"gpt-4o\",\n",
    "#         max_tokens=1000,\n",
    "#         temperature=0.4\n",
    "#     )\n",
    "\n",
    "#     return response # return to front-end in json format\n",
    "\n",
    "# response = RAG_pipeline(image_path, patient_hist)\n",
    "# print(\"Response:\")\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main pipeline (GPT only)\n",
    "1. Get \"image_path\" and \"patient_hist\" from frontend\n",
    "2. Send \"patient_hist\" and \"image_path\" to GPT and get \"response\" string in json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import json\n",
    "# import chromadb\n",
    "# from chromadb.utils.embedding_functions import OpenCLIPEmbeddingFunction\n",
    "# from chromadb.utils.data_loaders import ImageLoader\n",
    "# import base64\n",
    "# from openai import OpenAI\n",
    "# from PIL import Image\n",
    "# from io import BytesIO\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "# load_dotenv() # Load environment variables from .env file\n",
    "# api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# # Initialize vectorDB\n",
    "# multimodal_ef = OpenCLIPEmbeddingFunction() # multimodal embedding function\n",
    "# image_loader = ImageLoader() # multimodal data loader\n",
    "# client = chromadb.PersistentClient(path=\"myDB\") # persistent DB\n",
    "# multimodalDB = client.get_or_create_collection(name=\"multimodalDB\", embedding_function= multimodal_ef, data_loader=image_loader) # multimodal collection\n",
    "\n",
    "\n",
    "# # [FRONT-END] User input here\n",
    "# image_path = \"/Users/shinherng/Downloads/skinCond3.jpg\"\n",
    "# patient_hist = \"\"\"This lady, in her mid-20s, has a slowly spreading plaque on her forehead.\n",
    "#     Her general health is good. Biopsy has excluded malignancy, it shows hyperkeratosis,\n",
    "#     follicular plugging, basal keratinocytes degeneration, and a dense perivascular chronic inflammatory infiltrate.\"\"\"\n",
    "\n",
    "# # Main pipeline function\n",
    "# def GPT_pipeline(image_path, patient_hist):\n",
    "\n",
    "#     response = query_openai_with_image_and_text(\n",
    "#         text_prompt=patient_hist,\n",
    "#         image_source=image_path,\n",
    "#         api_key=api_key,\n",
    "#         model=\"gpt-4o\",\n",
    "#         max_tokens=1000,\n",
    "#         temperature=0.4\n",
    "#     )\n",
    "\n",
    "#     return response # return to front-end in json format\n",
    "\n",
    "# response = GPT_pipeline(image_path, patient_hist)\n",
    "# print(\"Response:\")\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages and Initialisation of DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import json\n",
    "# import chromadb\n",
    "# from chromadb.utils.embedding_functions import OpenCLIPEmbeddingFunction\n",
    "# from chromadb.utils.data_loaders import ImageLoader\n",
    "# import base64\n",
    "# from openai import OpenAI\n",
    "# from PIL import Image\n",
    "# from io import BytesIO\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "# load_dotenv() # Load environment variables from .env file\n",
    "# api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# multimodal_ef = OpenCLIPEmbeddingFunction() # multimodal embedding function\n",
    "# image_loader = ImageLoader() # multimodal data loader\n",
    "# # client = chromadb.Client() # non-persistent DB\n",
    "# client = chromadb.PersistentClient(path=\"myDB\") # persistent DB\n",
    "\n",
    "# multimodalDB = client.get_or_create_collection(name=\"multimodalDB\", embedding_function= multimodal_ef, data_loader=image_loader) # multimodal collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
