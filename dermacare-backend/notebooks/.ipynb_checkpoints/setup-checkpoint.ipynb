{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETUP\n",
    "Before running anything:\n",
    "1. Get openAI API key and store as \"OPENAI_API_KEY\" in .env file (create in working directory)\n",
    "2. Get Kaggle API json file (should be called Kaggle Settings.json) and store in \".kaggle\" folder (create in working directory)\n",
    "\n",
    "Running code below will:\n",
    "1. Download 2gb Dermnet dataset\n",
    "2. Create a json file called \"diagnosis_mapping.json\" that contains each image path and diagnosis\n",
    "3. Create a persistent vectorDB called \" using chroma\n",
    "4. Store image and metadata embeddings in IRISVectorDB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Just run this first, dont need to edit anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "import json\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "from PIL import Image\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"): \n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download the dataset from Kaggle (ensure kagglehub is installed and configured)\n",
    "# import kagglehub\n",
    "# dataset_path = kagglehub.dataset_download(\"shubhamgoel27/dermnet\")\n",
    "# print(\"Dataset downloaded to:\", dataset_path)\n",
    "\n",
    "dataset_path = \"../data/dermnet_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 5 image documents.\n"
     ]
    }
   ],
   "source": [
    "# --- Create Documents from Image Files ---\n",
    "def extract_diagnosis(filename):\n",
    "    \"\"\"\n",
    "    Extracts a diagnosis string from a filename by removing leading numbers/special characters\n",
    "    and capitalizing the remaining words.\n",
    "    \"\"\"\n",
    "    name = os.path.splitext(filename)[0]\n",
    "    cleaned = re.sub(r'^[\\d_\\-]+', '', name)\n",
    "    parts = re.split(r'[\\d_\\-]+', cleaned)\n",
    "    diagnosis = ' '.join(part.strip().capitalize() for part in parts if part.strip())\n",
    "    return diagnosis\n",
    "\n",
    "def create_documents_from_images(root_dir):\n",
    "    \"\"\"\n",
    "    Walk through the dataset directory and create a Document for each image.\n",
    "    The document's page_content is set to the image file path and its metadata contains the diagnosis.\n",
    "    \"\"\"\n",
    "    from langchain.docstore.document import Document\n",
    "    docs = []\n",
    "    for root, dirs, files in os.walk(root_dir):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                full_path = os.path.join(root, file)\n",
    "                diagnosis = extract_diagnosis(file)\n",
    "                doc = Document(page_content=full_path, metadata={\"diagnosis\": diagnosis, \"path\": full_path})\n",
    "                docs.append(doc)\n",
    "    return docs\n",
    "\n",
    "docs = create_documents_from_images(dataset_path)\n",
    "print(f\"Created {len(docs)} image documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d394549409e44981a09ffb7ca800dcd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "open_clip_model.safetensors:   0%|          | 0.00/5.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1 with 1000 documents...\n",
      "Batch 1 processed.\n",
      "Processing batch 2 with 1000 documents...\n",
      "Batch 2 processed.\n",
      "Processing batch 3 with 1000 documents...\n",
      "Batch 3 processed.\n",
      "Processing batch 4 with 1000 documents...\n",
      "Batch 4 processed.\n",
      "Processing batch 5 with 1000 documents...\n",
      "Batch 5 processed.\n",
      "Processing batch 6 with 1000 documents...\n",
      "Batch 6 processed.\n",
      "Processing batch 7 with 1000 documents...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     35\u001b[39m         db = IRISVector.from_documents(\n\u001b[32m     36\u001b[39m             embedding=multimodal_ef,\n\u001b[32m     37\u001b[39m             documents=batch_docs,\n\u001b[32m     38\u001b[39m             collection_name=COLLECTION_NAME,\n\u001b[32m     39\u001b[39m             connection_string=CONNECTION_STRING,\n\u001b[32m     40\u001b[39m         )\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     42\u001b[39m         \u001b[38;5;66;03m# For subsequent batches, add documents to the existing store.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m         \u001b[43mdb\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_docs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m processed.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# After processing, you can check the total number of documents.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Programming\\dermacare\\dermacare-backend\\iris-env\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:287\u001b[39m, in \u001b[36mVectorStore.add_documents\u001b[39m\u001b[34m(self, documents, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m     texts = [doc.page_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[32m    286\u001b[39m     metadatas = [doc.metadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    288\u001b[39m msg = (\n\u001b[32m    289\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m`add_documents` and `add_texts` has not been implemented \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    290\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    291\u001b[39m )\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Programming\\dermacare\\dermacare-backend\\iris-env\\Lib\\site-packages\\langchain_iris\\vectorstores.py:446\u001b[39m, in \u001b[36mIRISVector.add_texts\u001b[39m\u001b[34m(self, texts, metadatas, ids, batch_size, **kwargs)\u001b[39m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34madd_texts\u001b[39m(\n\u001b[32m    429\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    430\u001b[39m     texts: Iterable[\u001b[38;5;28mstr\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    434\u001b[39m     **kwargs: Any,\n\u001b[32m    435\u001b[39m ) -> List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    436\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Run more texts through the embeddings and add to the vectorstore.\u001b[39;00m\n\u001b[32m    437\u001b[39m \n\u001b[32m    438\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    444\u001b[39m \u001b[33;03m        List of ids from adding the texts into the vectorstore.\u001b[39;00m\n\u001b[32m    445\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m446\u001b[39m     embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membedding_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    448\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.add_embeddings(\n\u001b[32m    449\u001b[39m         texts=texts,\n\u001b[32m    450\u001b[39m         embeddings=embeddings,\n\u001b[32m   (...)\u001b[39m\u001b[32m    454\u001b[39m         **kwargs,\n\u001b[32m    455\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Programming\\dermacare\\dermacare-backend\\iris-env\\Lib\\site-packages\\langchain_experimental\\open_clip\\open_clip.py:54\u001b[39m, in \u001b[36mOpenCLIPEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts)\u001b[39m\n\u001b[32m     51\u001b[39m tokenized_text = \u001b[38;5;28mself\u001b[39m.tokenizer(text)\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# Encode the text to get the embeddings\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m embeddings_tensor = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# Normalize the embeddings\u001b[39;00m\n\u001b[32m     57\u001b[39m norm = embeddings_tensor.norm(p=\u001b[32m2\u001b[39m, dim=\u001b[32m1\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Programming\\dermacare\\dermacare-backend\\iris-env\\Lib\\site-packages\\open_clip\\model.py:288\u001b[39m, in \u001b[36mCLIP.encode_text\u001b[39m\u001b[34m(self, text, normalize)\u001b[39m\n\u001b[32m    285\u001b[39m x = \u001b[38;5;28mself\u001b[39m.token_embedding(text).to(cast_dtype)  \u001b[38;5;66;03m# [batch_size, n_ctx, d_model]\u001b[39;00m\n\u001b[32m    287\u001b[39m x = x + \u001b[38;5;28mself\u001b[39m.positional_embedding.to(cast_dtype)\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    289\u001b[39m x = \u001b[38;5;28mself\u001b[39m.ln_final(x)  \u001b[38;5;66;03m# [batch_size, n_ctx, transformer.width]\u001b[39;00m\n\u001b[32m    290\u001b[39m x, _ = text_global_pool(x, text, \u001b[38;5;28mself\u001b[39m.text_pool_type)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Programming\\dermacare\\dermacare-backend\\iris-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Programming\\dermacare\\dermacare-backend\\iris-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Programming\\dermacare\\dermacare-backend\\iris-env\\Lib\\site-packages\\open_clip\\transformer.py:363\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, x, attn_mask)\u001b[39m\n\u001b[32m    361\u001b[39m         x = checkpoint(r, x, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, attn_mask)\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m         x = \u001b[43mr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.batch_first:\n\u001b[32m    365\u001b[39m     x = x.transpose(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)    \u001b[38;5;66;03m# LND -> NLD\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Programming\\dermacare\\dermacare-backend\\iris-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Programming\\dermacare\\dermacare-backend\\iris-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Programming\\dermacare\\dermacare-backend\\iris-env\\Lib\\site-packages\\open_clip\\transformer.py:264\u001b[39m, in \u001b[36mResidualAttentionBlock.forward\u001b[39m\u001b[34m(self, q_x, k_x, v_x, attn_mask)\u001b[39m\n\u001b[32m    262\u001b[39m v_x = \u001b[38;5;28mself\u001b[39m.ln_1_kv(v_x) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mln_1_kv\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m v_x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    263\u001b[39m x = q_x + \u001b[38;5;28mself\u001b[39m.ls_1(\u001b[38;5;28mself\u001b[39m.attention(q_x=\u001b[38;5;28mself\u001b[39m.ln_1(q_x), k_x=k_x, v_x=v_x, attn_mask=attn_mask))\n\u001b[32m--> \u001b[39m\u001b[32m264\u001b[39m x = x + \u001b[38;5;28mself\u001b[39m.ls_2(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mln_2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Programming\\dermacare\\dermacare-backend\\iris-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Programming\\dermacare\\dermacare-backend\\iris-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Programming\\dermacare\\dermacare-backend\\iris-env\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Programming\\dermacare\\dermacare-backend\\iris-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Programming\\dermacare\\dermacare-backend\\iris-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Programming\\dermacare\\dermacare-backend\\iris-env\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from langchain_experimental.open_clip import OpenCLIPEmbeddings\n",
    "from langchain_iris import IRISVector\n",
    "import os\n",
    "\n",
    "# Initialize the image embedding function with your chosen model and checkpoint.\n",
    "multimodal_ef = OpenCLIPEmbeddings(model_name=\"ViT-g-14\", checkpoint=\"laion2b_s34b_b88k\")\n",
    "\n",
    "# Define your IRIS connection parameters.\n",
    "username = 'demo'\n",
    "password = 'demo'\n",
    "hostname = os.getenv('IRIS_HOSTNAME', 'localhost')\n",
    "port = '1972'\n",
    "namespace = 'USER'\n",
    "CONNECTION_STRING = f\"iris://{username}:{password}@{hostname}:{port}/{namespace}\"\n",
    "\n",
    "# Choose a collection name (avoid periods as it becomes a SQL table name).\n",
    "COLLECTION_NAME = \"dermnet_multimodal\"\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Create or update the IRISVector persistent vector store with the small batch of image documents.\n",
    "db = IRISVector.from_documents(\n",
    "    embedding=multimodal_ef,\n",
    "    documents=docs,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection_string=CONNECTION_STRING,\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "print(\"Vector store created with image embeddings for the small batch.\")\n",
    "ids = db.get().get(\"ids\", [])\n",
    "print(f\"Number of docs in vector store: {len(ids)}\")\n",
    "print(f\"Time taken: {elapsed:.2f} seconds\")\n",
    "\n",
    "# def batch(iterable, n=1):\n",
    "#     \"\"\"Yield successive n-sized batches from iterable.\"\"\"\n",
    "#     for i in range(0, len(iterable), n):\n",
    "#         yield iterable[i:i + n]\n",
    "\n",
    "# # Process docs in batches of 1000\n",
    "# batch_size = 1000\n",
    "\n",
    "# # For the first batch, create the store.\n",
    "# for i, batch_docs in enumerate(batch(docs, batch_size)):\n",
    "#     print(f\"Processing batch {i+1} with {len(batch_docs)} documents...\")\n",
    "#     if i == 0:\n",
    "#         # Create the store for the first batch.\n",
    "#         db = IRISVector.from_documents(\n",
    "#             embedding=multimodal_ef,\n",
    "#             documents=batch_docs,\n",
    "#             collection_name=COLLECTION_NAME,\n",
    "#             connection_string=CONNECTION_STRING,\n",
    "#         )\n",
    "#     else:\n",
    "#         # For subsequent batches, add documents to the existing store.\n",
    "#         db.add_documents(batch_docs)\n",
    "#     print(f\"Batch {i+1} processed.\")\n",
    "\n",
    "# # After processing, you can check the total number of documents.\n",
    "# elapsed = time.time() - start\n",
    "# ids = db.get().get(\"ids\", [])\n",
    "# print(f\"Total number of docs in vector store: {len(ids)}\")\n",
    "# print(f\"Time taken: {elapsed:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top similar results:\n",
      "Diagnosis: Cheilitis | Path: C:\\Users\\tanhu\\.cache\\kagglehub\\datasets\\shubhamgoel27\\dermnet\\versions\\1\\test\\Tinea Ringworm Candidiasis and other Fungal Infections\\03cheilitis05010422.jpg\n",
      "Diagnosis: Herpes Mouth | Path: C:\\Users\\tanhu\\.cache\\kagglehub\\datasets\\shubhamgoel27\\dermnet\\versions\\1\\test\\Warts Molluscum and other Viral Infections\\herpes-mouth-11.jpg\n",
      "Diagnosis: Herpes Gestationis | Path: C:\\Users\\tanhu\\.cache\\kagglehub\\datasets\\shubhamgoel27\\dermnet\\versions\\1\\test\\Bullous Disease Photos\\herpes-gestationis-12.jpg\n"
     ]
    }
   ],
   "source": [
    "# --- Query the Vector Store Using an Image ---\n",
    "# Update this path to point to your query image.\n",
    "query_image_path = \"../data/test.jpg\"\n",
    "# Embed the query image. Note: embed_image expects a list of image URIs.\n",
    "query_embedding = multimodal_ef.embed_image([query_image_path])[0]\n",
    "\n",
    "# Perform a similarity search using the query image's embedding vector.\n",
    "results = db.similarity_search_by_vector(query_embedding, k=3)\n",
    "\n",
    "print(\"Top similar results:\")\n",
    "for result in results:\n",
    "    diagnosis = result.metadata.get(\"diagnosis\", \"N/A\")\n",
    "    path = result.metadata.get(\"path\", \"N/A\")\n",
    "    print(f\"Diagnosis: {diagnosis} | Path: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top similar results:\n",
      "Diagnosis: Rhinophyma | Path: C:\\Users\\tanhu\\.cache\\kagglehub\\datasets\\shubhamgoel27\\dermnet\\versions\\1\\test\\Acne and Rosacea Photos\\07Rhinophyma1.jpg\n",
      "Diagnosis: Rhinophyma | Path: C:\\Users\\tanhu\\.cache\\kagglehub\\datasets\\shubhamgoel27\\dermnet\\versions\\1\\test\\Acne and Rosacea Photos\\07Rhinophyma1.jpg\n",
      "Diagnosis: Rhinophyma | Path: C:\\Users\\tanhu\\.cache\\kagglehub\\datasets\\shubhamgoel27\\dermnet\\versions\\1\\test\\Acne and Rosacea Photos\\07Rhinophyma1.jpg\n"
     ]
    }
   ],
   "source": [
    "# ## REQUERY\n",
    "# from langchain_experimental.open_clip import OpenCLIPEmbeddings\n",
    "# from langchain_iris import IRISVector\n",
    "# import os\n",
    "\n",
    "# # Initialize the embedding function\n",
    "# multimodal_ef = OpenCLIPEmbeddings(model_name=\"ViT-g-14\", checkpoint=\"laion2b_s34b_b88k\")\n",
    "\n",
    "# # Set up your connection parameters again\n",
    "# username = 'demo'\n",
    "# password = 'demo'\n",
    "# hostname = os.getenv('IRIS_HOSTNAME', 'localhost')\n",
    "# port = '1972'\n",
    "# namespace = 'USER'\n",
    "# CONNECTION_STRING = f\"iris://{username}:{password}@{hostname}:{port}/{namespace}\"\n",
    "# COLLECTION_NAME = \"dermnet_multimodal\"\n",
    "\n",
    "# # Load the persistent vector store\n",
    "# db = IRISVector.from_documents(\n",
    "#     embedding=multimodal_ef,\n",
    "#     documents=[],  # Passing an empty list so that it doesn't re-embed\n",
    "#     collection_name=COLLECTION_NAME,\n",
    "#     connection_string=CONNECTION_STRING,\n",
    "# )\n",
    "\n",
    "# # --- Query the Vector Store Using an Image ---\n",
    "# query_image_path = \"../data/test.jpg\"\n",
    "# query_embedding = multimodal_ef.embed_image([query_image_path])[0]\n",
    "\n",
    "# results = db.similarity_search_by_vector(query_embedding, k=3)\n",
    "\n",
    "# print(\"Top similar results:\")\n",
    "# for result in results:\n",
    "#     diagnosis = result.metadata.get(\"diagnosis\", \"N/A\")\n",
    "#     path = result.metadata.get(\"path\", \"N/A\")\n",
    "#     print(f\"Diagnosis: {diagnosis} | Path: {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
